{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 加载数据\n",
    "2. 切片\n",
    "3. 删除无信息（安静）片段\n",
    "4. 精简数据（how？）\n",
    "5. fft\n",
    "6. write into static dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SpeechRecognition\n",
    "# !pip install soundfile\n",
    "# ## !pip install cuml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "from glob import glob\n",
    "\n",
    "import soundfile as sf            # To read .flac files.   \n",
    "import speech_recognition as sr   # pip install SpeechRecognition.\n",
    "from sklearn.manifold import TSNE\n",
    "# import cuml\n",
    "# import cudf\n",
    "# from cuml.manifold import TSNE as cudaTSNE\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# static parameters\n",
    "durationCheck = 10.      # Only consider files with 10 or more seconds of audio.\n",
    "deltaT        = 0.2      # Audio frame size is 0.2 seconds.\n",
    "noisy         = 0.1      # This sets the limit for static, i.e. pauses in speech.\n",
    "lim1 = 10; lim2 = 410    # Lower and upper frequencies. \n",
    "                         # For the above parameters and 16 kHz sampling, this range is about 50 - 2000 Hz.  \n",
    "\n",
    "fft_numFeatures = lim2-lim1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the current directory\n",
    "proj_root_dir = os.path.abspath(os.path.join(os.getcwd(),\"..\"))\n",
    "\n",
    "target_folder_dir = os.path.join(proj_root_dir, 'data', 'LibriSpeech-SI')\n",
    "state_dict_folder_dir = os.path.join(proj_root_dir, 'state_dict')\n",
    "division = 'test'\n",
    "target_folder_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fft calculation\n",
    "\n",
    "# Read data from a folder into a list.\n",
    "def getSpeakerData(division, file_name):\n",
    "  flac_file_dir = glob(os.path.join(target_folder_dir, division, file_name))[0]\n",
    "  flac_dir_list = []\n",
    "  data,samplerate = sf.read(flac_file_dir)  \n",
    "  duration = len(data)*1./samplerate\n",
    "  if duration >= durationCheck: \n",
    "    flac_dir_list.append(flac_file_dir)\n",
    "\n",
    "  chunksF = []\n",
    "  speaker_fft = []\n",
    "\n",
    "  for flac_dir in flac_dir_list:\n",
    "    # print(\"flac_dir: \", flac_dir)\n",
    "    data,samplerate = sf.read(flac_dir)  \n",
    "    duration = len(data)*1./samplerate\n",
    "\n",
    "    # Divide audio data into frames, or chunks. \n",
    "    numChunks = int(duration/deltaT)\n",
    "    sizeChunk = int(len(data)/numChunks)\n",
    "    for lp in range(0,numChunks):    \n",
    "      chunk = data[lp*sizeChunk:(lp+1)*sizeChunk]      # get a chunk of speech.     \n",
    "      chunksF.append(np.abs(np.fft.rfft(chunk))[lim1:lim2])  # take the FFT.\n",
    "      # shape of chunksF: slice_num * 400\n",
    "\n",
    "    # Delete quiet parts of speech, i.e. pauses.\n",
    "    # Most of the power is in the bottom 50% of frequencies.\n",
    "    mu = np.mean([np.mean(chunksF[i][:fft_numFeatures//2]) for i in range(0,len(chunksF))])\n",
    "    # speaker_fft = []\n",
    "    for chunkF in chunksF:\n",
    "      if np.mean(chunkF[:fft_numFeatures//2]) > noisy*mu:\n",
    "        speaker_fft.append(chunkF)\n",
    "        # shape of speaker_fft: slice_w_information_num * 400\n",
    "    \n",
    "  return speaker_fft\n",
    "\n",
    "# Return data for all speakers.\n",
    "def getDataSpeakers(division):\n",
    "  dataSpeakers = []\n",
    "  speaker_info = []\n",
    "  for speaker in tqdm(sorted(os.listdir(os.path.join(target_folder_dir, division)))):\n",
    "    # print (\"Getting data for speaker: \"+speaker)\n",
    "    dataSpeakers.append(getSpeakerData(division, speaker))\n",
    "    # shape of dataSpeakers: speaker_num * slice_w_info_num * 400\n",
    "    speaker_info.append(speaker.split(\".\")[0])\n",
    "  # print(type(dataSpeakers))\n",
    "  N = np.sum([np.shape(s)[0] for s in dataSpeakers])\n",
    "  tX = np.mat(np.zeros((N,fft_numFeatures)))\n",
    "  tY = []\n",
    "  speakerIndices = [0]    # Index corresponding to start of speaker 'n'\n",
    "  \n",
    "  ctr = 0; lp = 0\n",
    "  for dataSpeaker in dataSpeakers:\n",
    "    for j in range(0,len(dataSpeaker)):\n",
    "      for k in range(0,fft_numFeatures):\n",
    "        tX[ctr,k] = dataSpeaker[j][k]\n",
    "      # tY.append(lp)\n",
    "      tY.append(speaker_info[lp])\n",
    "      ctr += 1  \n",
    "    speakerIndices.append(ctr)\n",
    "    lp += 1  \n",
    "          \n",
    "  return tX,tY,speakerIndices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall fft information\n",
    "trnX,trnY,trnIdx = getDataSpeakers(division)\n",
    "print(np.shape(trnX), np.shape(trnY))\n",
    "trnRows = np.shape(trnX)[0]\n",
    "# print(trnIdx)   # Start location of speaker 'i'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_folder_dir = os.path.join(state_dict_folder_dir, 'fft_result_wo_simplify', division)\n",
    "try:\n",
    "    os.mkdir(save_folder_dir)\n",
    "except OSError:\n",
    "    pass\n",
    "\n",
    "with open(os.path.join(save_folder_dir, 'trnX.pkl'), 'wb') as f:\n",
    "    pkl.dump(trnX, f)\n",
    "with open(os.path.join(save_folder_dir, 'trnY.pkl'), 'wb') as f:\n",
    "    pkl.dump(trnY, f)\n",
    "with open(os.path.join(save_folder_dir, 'trnIdx.pkl'), 'wb') as f:\n",
    "    pkl.dump(trnIdx, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # t-SNE dimentional reduction\n",
    "\n",
    "# with open('/data1/home/xiruiling/course/AdvanceArtificialIntelligence/AAI_Proj/state_dict/fft_result_wo_simplify/trnX.pkl', 'rb') as f:\n",
    "#     trnX = pkl.load(f)\n",
    "# print(\"The original dimension: \", np.array(trnX).shape)\n",
    "# x_tsne = TSNE(n_components=2,random_state=0).fit_transform(trnX)\n",
    "# # x_tsne = cudaTSNE(n_components=2, perplexity=50, learning_rate=20).fit_transform(trnX)\n",
    "# x_tsne\n",
    "# # print(\"after t-SNE: \", x_tsne.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TestEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "638fd99b6f8315570d0eadb3c3caa9c92c17ee6381122f56dcae46cd26d849db"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}