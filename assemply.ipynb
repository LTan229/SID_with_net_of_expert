{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'最后在这里汇总所有代码'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''最后在这里汇总所有代码'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "'''train_single_col'''\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "def train_single_col(model, args, train_loader, test_loader):\n",
    "    device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr = args.lr,\n",
    "                                 weight_decay = args.weight_decay)\n",
    "    # Utils  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch_idx, (data, label) in enumerate(train_loader):\n",
    "\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.reshape(batch_size, -1)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model.forward(data)\n",
    "            output = output.reshape(batch_size, -1)\n",
    "            # output = F.softmax(output, dim=1)\n",
    "            loss = criterion(output, label.long().reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(optimizer.param_groups[0]['params'])\n",
    "\n",
    "            # Print training status\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                pred = output.data.max(1)[1]\n",
    "                correct = pred.eq(label.view(-1).data).sum()\n",
    "\n",
    "                msg = (\n",
    "                    \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                    \" Correct: {:03d}/{:03d}\"\n",
    "                )\n",
    "                print(msg.format(epoch, batch_idx, loss, correct, batch_size))\n",
    "                model.training_loss_list.append(loss.cpu().data.numpy())\n",
    "\n",
    "        # Evaluating\n",
    "        model.eval()\n",
    "        correct = 0.\n",
    "\n",
    "        for batch_idx, (data, label) in enumerate(test_loader):\n",
    "\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.reshape(batch_size, -1)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model.forward(data)\n",
    "            output = output.reshape(batch_size, -1)\n",
    "            output = F.softmax(output, dim=1)\n",
    "\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(label.view(-1).data).sum()\n",
    "\n",
    "        accuracy = 100.0 * float(correct) / len(test_loader.dataset)\n",
    "\n",
    "        model.testing_acc_list.append(accuracy)\n",
    "\n",
    "        if accuracy > model.best_testing_acc:\n",
    "            model.best_testing_acc = accuracy\n",
    "            torch.save(model, f'model/{args.name}.pt')\n",
    "\n",
    "        msg = (\n",
    "            \"\\nEpoch: {:02d} | Testing Accuracy: {}/{} ({:.3f}%) | \"\n",
    "            \" Historical Best: {:.3f}%\\n\"\n",
    "        )\n",
    "        print(\n",
    "            msg.format(\n",
    "                epoch, correct, len(test_loader.dataset), accuracy,\n",
    "                model.best_testing_acc\n",
    "            )\n",
    "        , flush=True)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    best_model = torch.load(f'model/{args.name}.pt')\n",
    "    best_model.testing_acc_list = model.testing_acc_list\n",
    "    torch.save(best_model, f'model/{args.name}.pt')\n",
    "    return max(model.testing_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_two_backbone'''\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "def train_two_backbone(model, args, train_loader, test_loader):\n",
    "    device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr = args.lr,\n",
    "                                 weight_decay = args.weight_decay)\n",
    "    # Utils  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch_idx, (data, _, label) in enumerate(train_loader):\n",
    "\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.reshape(batch_size, -1)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model.forward(data)\n",
    "            output = output.reshape(batch_size, -1)\n",
    "            # output = F.softmax(output, dim=1)\n",
    "            loss = criterion(output, label.long().reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(optimizer.param_groups[0]['params'])\n",
    "\n",
    "            # Print training status\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                pred = output.data.max(1)[1]\n",
    "                correct = pred.eq(label.view(-1).data).sum()\n",
    "\n",
    "                msg = (\n",
    "                    \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                    \" Correct: {:03d}/{:03d}\"\n",
    "                )\n",
    "                print(msg.format(epoch, batch_idx, loss, correct, batch_size))\n",
    "                model.training_loss_list.append(loss.cpu().data.numpy())\n",
    "\n",
    "        # Evaluating\n",
    "        model.eval()\n",
    "        correct = 0.\n",
    "\n",
    "        for batch_idx, (data, _, label) in enumerate(test_loader):\n",
    "\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.reshape(batch_size, -1)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model.forward(data)\n",
    "            output = output.reshape(batch_size, -1)\n",
    "            output = F.softmax(output, dim=1)\n",
    "\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(label.view(-1).data).sum()\n",
    "\n",
    "        accuracy = 100.0 * float(correct) / len(test_loader.dataset)\n",
    "\n",
    "        model.testing_acc_list.append(accuracy)\n",
    "\n",
    "        if accuracy > model.best_testing_acc:\n",
    "            model.best_testing_acc = accuracy\n",
    "            torch.save(model, f'model/{args.name}.pt')\n",
    "\n",
    "        msg = (\n",
    "            \"\\nEpoch: {:02d} | Testing Accuracy: {}/{} ({:.3f}%) | \"\n",
    "            \" Historical Best: {:.3f}%\\n\"\n",
    "        )\n",
    "        print(\n",
    "            msg.format(\n",
    "                epoch, correct, len(test_loader.dataset), accuracy,\n",
    "                model.best_testing_acc\n",
    "            )\n",
    "        , flush=True)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    best_model = torch.load(f'model/{args.name}.pt')\n",
    "    best_model.testing_acc_list = model.testing_acc_list\n",
    "    torch.save(best_model, f'model/{args.name}.pt')\n",
    "    return max(model.testing_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_two_specialty'''\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "def train_two_specialty(model, args, train_loader, test_loader):\n",
    "    device = torch.device(\"cuda\" if args.use_cuda else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr = args.lr,\n",
    "                                 weight_decay = args.weight_decay)\n",
    "    # Utils  \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "\n",
    "        # Training\n",
    "        model.train()\n",
    "        for batch_idx, (data, label, _) in enumerate(train_loader):\n",
    "\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.reshape(batch_size, -1)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model.forward(data)\n",
    "            output = output.reshape(batch_size, -1)\n",
    "            # output = F.softmax(output, dim=1)\n",
    "            loss = criterion(output, label.long().reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print(optimizer.param_groups[0]['params'])\n",
    "\n",
    "            # Print training status\n",
    "            if batch_idx % args.log_interval == 0:\n",
    "                pred = output.data.max(1)[1]\n",
    "                correct = pred.eq(label.view(-1).data).sum()\n",
    "\n",
    "                msg = (\n",
    "                    \"Epoch: {:02d} | Batch: {:03d} | Loss: {:.5f} |\"\n",
    "                    \" Correct: {:03d}/{:03d}\"\n",
    "                )\n",
    "                print(msg.format(epoch, batch_idx, loss, correct, batch_size))\n",
    "                model.training_loss_list.append(loss.cpu().data.numpy())\n",
    "\n",
    "        # Evaluating\n",
    "        model.eval()\n",
    "        correct = 0.\n",
    "\n",
    "        for batch_idx, (data, label, _) in enumerate(test_loader):\n",
    "\n",
    "            batch_size = data.size()[0]\n",
    "            data = data.reshape(batch_size, -1)\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            output = model.forward(data)\n",
    "            output = output.reshape(batch_size, -1)\n",
    "            output = F.softmax(output, dim=1)\n",
    "\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(label.view(-1).data).sum()\n",
    "\n",
    "        accuracy = 100.0 * float(correct) / len(test_loader.dataset)\n",
    "\n",
    "        model.testing_acc_list.append(accuracy)\n",
    "\n",
    "        if accuracy > model.best_testing_acc:\n",
    "            model.best_testing_acc = accuracy\n",
    "            torch.save(model, f'model/{args.name}.pt')\n",
    "\n",
    "        msg = (\n",
    "            \"\\nEpoch: {:02d} | Testing Accuracy: {}/{} ({:.3f}%) | \"\n",
    "            \" Historical Best: {:.3f}%\\n\"\n",
    "        )\n",
    "        print(\n",
    "            msg.format(\n",
    "                epoch, correct, len(test_loader.dataset), accuracy,\n",
    "                model.best_testing_acc\n",
    "            )\n",
    "        , flush=True)\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    best_model = torch.load(f'model/{args.name}.pt')\n",
    "    best_model.testing_acc_list = model.testing_acc_list\n",
    "    torch.save(best_model, f'model/{args.name}.pt')\n",
    "    return max(model.testing_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 127] 找不到指定的程序。\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 00 | Batch: 000 | Loss: 5.51817 | Correct: 000/128\n",
      "Epoch: 00 | Batch: 100 | Loss: 4.35929 | Correct: 010/128\n",
      "Epoch: 00 | Batch: 200 | Loss: 4.21655 | Correct: 010/128\n",
      "Epoch: 00 | Batch: 300 | Loss: 4.24636 | Correct: 011/128\n",
      "Epoch: 00 | Batch: 400 | Loss: 3.92718 | Correct: 013/128\n",
      "Epoch: 00 | Batch: 500 | Loss: 3.98968 | Correct: 013/128\n",
      "Epoch: 00 | Batch: 600 | Loss: 3.71097 | Correct: 021/128\n",
      "Epoch: 00 | Batch: 700 | Loss: 3.88653 | Correct: 015/128\n",
      "Epoch: 00 | Batch: 800 | Loss: 3.88194 | Correct: 019/128\n",
      "Epoch: 00 | Batch: 900 | Loss: 3.93740 | Correct: 019/128\n",
      "Epoch: 00 | Batch: 1000 | Loss: 3.82402 | Correct: 016/128\n",
      "Epoch: 00 | Batch: 1100 | Loss: 3.67279 | Correct: 023/128\n",
      "Epoch: 00 | Batch: 1200 | Loss: 3.92581 | Correct: 015/128\n",
      "Epoch: 00 | Batch: 1300 | Loss: 3.62234 | Correct: 020/128\n",
      "Epoch: 00 | Batch: 1400 | Loss: 3.47280 | Correct: 023/128\n",
      "Epoch: 00 | Batch: 1500 | Loss: 3.49672 | Correct: 029/128\n",
      "Epoch: 00 | Batch: 1600 | Loss: 3.63348 | Correct: 029/128\n",
      "Epoch: 00 | Batch: 1700 | Loss: 3.77287 | Correct: 015/128\n",
      "Epoch: 00 | Batch: 1800 | Loss: 3.48138 | Correct: 017/128\n",
      "Epoch: 00 | Batch: 1900 | Loss: 3.47899 | Correct: 024/128\n",
      "Epoch: 00 | Batch: 2000 | Loss: 3.29915 | Correct: 029/128\n",
      "Epoch: 00 | Batch: 2100 | Loss: 3.39222 | Correct: 022/128\n",
      "Epoch: 00 | Batch: 2200 | Loss: 3.63073 | Correct: 023/128\n",
      "Epoch: 00 | Batch: 2300 | Loss: 3.45688 | Correct: 022/128\n",
      "Epoch: 00 | Batch: 2400 | Loss: 3.46198 | Correct: 026/128\n",
      "Epoch: 00 | Batch: 2500 | Loss: 2.93804 | Correct: 037/128\n",
      "Epoch: 00 | Batch: 2600 | Loss: 3.33218 | Correct: 030/128\n",
      "Epoch: 00 | Batch: 2700 | Loss: 3.40021 | Correct: 025/128\n",
      "Epoch: 00 | Batch: 2800 | Loss: 3.32892 | Correct: 029/128\n",
      "Epoch: 00 | Batch: 2900 | Loss: 3.25449 | Correct: 025/128\n",
      "Epoch: 00 | Batch: 3000 | Loss: 3.41751 | Correct: 021/128\n",
      "Epoch: 00 | Batch: 3100 | Loss: 3.60427 | Correct: 024/128\n",
      "Epoch: 00 | Batch: 3200 | Loss: 3.14021 | Correct: 036/128\n",
      "Epoch: 00 | Batch: 3300 | Loss: 3.43232 | Correct: 021/128\n",
      "Epoch: 00 | Batch: 3400 | Loss: 3.19597 | Correct: 043/128\n",
      "Epoch: 00 | Batch: 3500 | Loss: 3.28723 | Correct: 026/128\n",
      "Epoch: 00 | Batch: 3600 | Loss: 3.30712 | Correct: 028/128\n",
      "Epoch: 00 | Batch: 3700 | Loss: 3.16966 | Correct: 030/128\n",
      "Epoch: 00 | Batch: 3800 | Loss: 3.62905 | Correct: 021/128\n",
      "Epoch: 00 | Batch: 3900 | Loss: 3.05832 | Correct: 034/128\n",
      "Epoch: 00 | Batch: 4000 | Loss: 3.27630 | Correct: 021/128\n",
      "Epoch: 00 | Batch: 4100 | Loss: 3.02438 | Correct: 039/128\n",
      "Epoch: 00 | Batch: 4200 | Loss: 3.06442 | Correct: 031/128\n",
      "Epoch: 00 | Batch: 4300 | Loss: 3.21237 | Correct: 031/128\n",
      "Epoch: 00 | Batch: 4400 | Loss: 3.32142 | Correct: 029/128\n",
      "Epoch: 00 | Batch: 4500 | Loss: 3.23258 | Correct: 030/128\n",
      "Epoch: 00 | Batch: 4600 | Loss: 2.89731 | Correct: 038/128\n",
      "Epoch: 00 | Batch: 4700 | Loss: 3.15373 | Correct: 038/128\n",
      "Epoch: 00 | Batch: 4800 | Loss: 2.99099 | Correct: 045/128\n",
      "Epoch: 00 | Batch: 4900 | Loss: 3.07922 | Correct: 037/128\n",
      "Epoch: 00 | Batch: 5000 | Loss: 3.38319 | Correct: 034/128\n",
      "Epoch: 00 | Batch: 5100 | Loss: 3.05626 | Correct: 037/128\n",
      "Epoch: 00 | Batch: 5200 | Loss: 3.02119 | Correct: 035/128\n",
      "Epoch: 00 | Batch: 5300 | Loss: 3.05547 | Correct: 033/128\n",
      "Epoch: 00 | Batch: 5400 | Loss: 3.46096 | Correct: 030/128\n",
      "Epoch: 00 | Batch: 5500 | Loss: 3.07315 | Correct: 036/128\n",
      "\n",
      "Epoch: 00 | Testing Accuracy: 45343.0/178167 (25.450%) |  Historical Best: 25.450%\n",
      "\n",
      "Epoch: 01 | Batch: 000 | Loss: 3.19616 | Correct: 030/128\n",
      "Epoch: 01 | Batch: 100 | Loss: 2.93418 | Correct: 034/128\n",
      "Epoch: 01 | Batch: 200 | Loss: 3.09417 | Correct: 030/128\n",
      "Epoch: 01 | Batch: 300 | Loss: 3.25680 | Correct: 025/128\n",
      "Epoch: 01 | Batch: 400 | Loss: 2.66721 | Correct: 046/128\n",
      "Epoch: 01 | Batch: 500 | Loss: 3.20596 | Correct: 021/128\n",
      "Epoch: 01 | Batch: 600 | Loss: 3.02300 | Correct: 033/128\n",
      "Epoch: 01 | Batch: 700 | Loss: 3.03360 | Correct: 034/128\n",
      "Epoch: 01 | Batch: 800 | Loss: 2.90489 | Correct: 041/128\n",
      "Epoch: 01 | Batch: 900 | Loss: 2.91832 | Correct: 036/128\n",
      "Epoch: 01 | Batch: 1000 | Loss: 2.77365 | Correct: 039/128\n",
      "Epoch: 01 | Batch: 1100 | Loss: 2.92605 | Correct: 043/128\n",
      "Epoch: 01 | Batch: 1200 | Loss: 3.25571 | Correct: 030/128\n",
      "Epoch: 01 | Batch: 1300 | Loss: 3.15118 | Correct: 024/128\n",
      "Epoch: 01 | Batch: 1400 | Loss: 3.33797 | Correct: 030/128\n",
      "Epoch: 01 | Batch: 1500 | Loss: 2.94225 | Correct: 031/128\n",
      "Epoch: 01 | Batch: 1600 | Loss: 3.29876 | Correct: 029/128\n",
      "Epoch: 01 | Batch: 1700 | Loss: 2.81663 | Correct: 040/128\n",
      "Epoch: 01 | Batch: 1800 | Loss: 2.96354 | Correct: 035/128\n",
      "Epoch: 01 | Batch: 1900 | Loss: 3.26309 | Correct: 037/128\n",
      "Epoch: 01 | Batch: 2000 | Loss: 3.25387 | Correct: 032/128\n",
      "Epoch: 01 | Batch: 2100 | Loss: 3.26635 | Correct: 022/128\n",
      "Epoch: 01 | Batch: 2200 | Loss: 3.03354 | Correct: 027/128\n",
      "Epoch: 01 | Batch: 2300 | Loss: 3.12728 | Correct: 025/128\n",
      "Epoch: 01 | Batch: 2400 | Loss: 2.67262 | Correct: 048/128\n",
      "Epoch: 01 | Batch: 2500 | Loss: 2.91412 | Correct: 034/128\n",
      "Epoch: 01 | Batch: 2600 | Loss: 3.06393 | Correct: 031/128\n",
      "Epoch: 01 | Batch: 2700 | Loss: 3.15110 | Correct: 032/128\n",
      "Epoch: 01 | Batch: 2800 | Loss: 3.13907 | Correct: 032/128\n",
      "Epoch: 01 | Batch: 2900 | Loss: 3.11461 | Correct: 038/128\n",
      "Epoch: 01 | Batch: 3000 | Loss: 3.04163 | Correct: 038/128\n",
      "Epoch: 01 | Batch: 3100 | Loss: 3.18047 | Correct: 032/128\n",
      "Epoch: 01 | Batch: 3200 | Loss: 3.12919 | Correct: 030/128\n",
      "Epoch: 01 | Batch: 3300 | Loss: 3.24762 | Correct: 031/128\n",
      "Epoch: 01 | Batch: 3400 | Loss: 2.78106 | Correct: 043/128\n",
      "Epoch: 01 | Batch: 3500 | Loss: 2.86105 | Correct: 041/128\n",
      "Epoch: 01 | Batch: 3600 | Loss: 3.17394 | Correct: 029/128\n",
      "Epoch: 01 | Batch: 3700 | Loss: 2.84581 | Correct: 039/128\n",
      "Epoch: 01 | Batch: 3800 | Loss: 2.86385 | Correct: 039/128\n",
      "Epoch: 01 | Batch: 3900 | Loss: 2.95005 | Correct: 040/128\n",
      "Epoch: 01 | Batch: 4000 | Loss: 2.82948 | Correct: 038/128\n",
      "Epoch: 01 | Batch: 4100 | Loss: 3.16583 | Correct: 030/128\n",
      "Epoch: 01 | Batch: 4200 | Loss: 2.96325 | Correct: 027/128\n",
      "Epoch: 01 | Batch: 4300 | Loss: 2.89913 | Correct: 044/128\n",
      "Epoch: 01 | Batch: 4400 | Loss: 3.00588 | Correct: 036/128\n",
      "Epoch: 01 | Batch: 4500 | Loss: 2.98087 | Correct: 044/128\n",
      "Epoch: 01 | Batch: 4600 | Loss: 3.31780 | Correct: 030/128\n",
      "Epoch: 01 | Batch: 4700 | Loss: 2.80325 | Correct: 043/128\n",
      "Epoch: 01 | Batch: 4800 | Loss: 3.15105 | Correct: 029/128\n",
      "Epoch: 01 | Batch: 4900 | Loss: 3.08333 | Correct: 035/128\n",
      "Epoch: 01 | Batch: 5000 | Loss: 3.10363 | Correct: 038/128\n",
      "Epoch: 01 | Batch: 5100 | Loss: 3.00230 | Correct: 038/128\n",
      "Epoch: 01 | Batch: 5200 | Loss: 3.16596 | Correct: 035/128\n",
      "Epoch: 01 | Batch: 5300 | Loss: 2.86751 | Correct: 038/128\n",
      "Epoch: 01 | Batch: 5400 | Loss: 2.95847 | Correct: 035/128\n",
      "Epoch: 01 | Batch: 5500 | Loss: 2.78504 | Correct: 038/128\n",
      "\n",
      "Epoch: 01 | Testing Accuracy: 50582.0/178167 (28.390%) |  Historical Best: 28.390%\n",
      "\n",
      "Epoch: 02 | Batch: 000 | Loss: 2.70071 | Correct: 042/128\n",
      "Epoch: 02 | Batch: 100 | Loss: 3.06670 | Correct: 038/128\n",
      "Epoch: 02 | Batch: 200 | Loss: 3.11211 | Correct: 036/128\n",
      "Epoch: 02 | Batch: 300 | Loss: 2.89138 | Correct: 040/128\n",
      "Epoch: 02 | Batch: 400 | Loss: 2.90993 | Correct: 035/128\n",
      "Epoch: 02 | Batch: 500 | Loss: 3.21514 | Correct: 038/128\n",
      "Epoch: 02 | Batch: 600 | Loss: 3.00463 | Correct: 036/128\n",
      "Epoch: 02 | Batch: 700 | Loss: 3.12581 | Correct: 031/128\n",
      "Epoch: 02 | Batch: 800 | Loss: 3.06141 | Correct: 034/128\n",
      "Epoch: 02 | Batch: 900 | Loss: 3.25610 | Correct: 037/128\n",
      "Epoch: 02 | Batch: 1000 | Loss: 2.92411 | Correct: 042/128\n",
      "Epoch: 02 | Batch: 1100 | Loss: 2.84007 | Correct: 040/128\n",
      "Epoch: 02 | Batch: 1200 | Loss: 2.99403 | Correct: 036/128\n",
      "Epoch: 02 | Batch: 1300 | Loss: 2.89844 | Correct: 040/128\n",
      "Epoch: 02 | Batch: 1400 | Loss: 2.94813 | Correct: 039/128\n",
      "Epoch: 02 | Batch: 1500 | Loss: 2.88080 | Correct: 039/128\n",
      "Epoch: 02 | Batch: 1600 | Loss: 3.08792 | Correct: 035/128\n",
      "Epoch: 02 | Batch: 1700 | Loss: 2.96173 | Correct: 036/128\n",
      "Epoch: 02 | Batch: 1800 | Loss: 3.07058 | Correct: 026/128\n",
      "Epoch: 02 | Batch: 1900 | Loss: 3.02806 | Correct: 031/128\n",
      "Epoch: 02 | Batch: 2000 | Loss: 2.69830 | Correct: 043/128\n",
      "Epoch: 02 | Batch: 2100 | Loss: 2.96701 | Correct: 038/128\n",
      "Epoch: 02 | Batch: 2200 | Loss: 2.98355 | Correct: 033/128\n",
      "Epoch: 02 | Batch: 2300 | Loss: 2.89498 | Correct: 035/128\n",
      "Epoch: 02 | Batch: 2400 | Loss: 2.69241 | Correct: 041/128\n",
      "Epoch: 02 | Batch: 2500 | Loss: 2.74003 | Correct: 037/128\n",
      "Epoch: 02 | Batch: 2600 | Loss: 2.98962 | Correct: 035/128\n",
      "Epoch: 02 | Batch: 2700 | Loss: 2.95548 | Correct: 039/128\n",
      "Epoch: 02 | Batch: 2800 | Loss: 3.12701 | Correct: 030/128\n",
      "Epoch: 02 | Batch: 2900 | Loss: 2.85918 | Correct: 042/128\n",
      "Epoch: 02 | Batch: 3000 | Loss: 2.97335 | Correct: 035/128\n",
      "Epoch: 02 | Batch: 3100 | Loss: 2.76086 | Correct: 043/128\n",
      "Epoch: 02 | Batch: 3200 | Loss: 2.87257 | Correct: 043/128\n",
      "Epoch: 02 | Batch: 3300 | Loss: 2.89032 | Correct: 036/128\n",
      "Epoch: 02 | Batch: 3400 | Loss: 2.92101 | Correct: 033/128\n",
      "Epoch: 02 | Batch: 3500 | Loss: 2.93049 | Correct: 036/128\n",
      "Epoch: 02 | Batch: 3600 | Loss: 2.79255 | Correct: 036/128\n",
      "Epoch: 02 | Batch: 3700 | Loss: 2.76815 | Correct: 038/128\n",
      "Epoch: 02 | Batch: 3800 | Loss: 3.04935 | Correct: 039/128\n",
      "Epoch: 02 | Batch: 3900 | Loss: 2.88235 | Correct: 042/128\n",
      "Epoch: 02 | Batch: 4000 | Loss: 3.03539 | Correct: 038/128\n",
      "Epoch: 02 | Batch: 4100 | Loss: 2.88937 | Correct: 041/128\n",
      "Epoch: 02 | Batch: 4200 | Loss: 3.14091 | Correct: 034/128\n",
      "Epoch: 02 | Batch: 4300 | Loss: 3.05267 | Correct: 039/128\n",
      "Epoch: 02 | Batch: 4400 | Loss: 3.01123 | Correct: 030/128\n",
      "Epoch: 02 | Batch: 4500 | Loss: 2.89205 | Correct: 036/128\n",
      "Epoch: 02 | Batch: 4600 | Loss: 3.11338 | Correct: 028/128\n",
      "Epoch: 02 | Batch: 4700 | Loss: 3.06101 | Correct: 034/128\n",
      "Epoch: 02 | Batch: 4800 | Loss: 2.95958 | Correct: 043/128\n",
      "Epoch: 02 | Batch: 4900 | Loss: 2.96567 | Correct: 038/128\n",
      "Epoch: 02 | Batch: 5000 | Loss: 2.67352 | Correct: 044/128\n",
      "Epoch: 02 | Batch: 5100 | Loss: 3.13074 | Correct: 025/128\n",
      "Epoch: 02 | Batch: 5200 | Loss: 2.81897 | Correct: 043/128\n",
      "Epoch: 02 | Batch: 5300 | Loss: 2.97138 | Correct: 038/128\n",
      "Epoch: 02 | Batch: 5400 | Loss: 3.05267 | Correct: 034/128\n",
      "Epoch: 02 | Batch: 5500 | Loss: 3.06856 | Correct: 029/128\n",
      "\n",
      "Epoch: 02 | Testing Accuracy: 52546.0/178167 (29.493%) |  Historical Best: 29.493%\n",
      "\n",
      "Epoch: 03 | Batch: 000 | Loss: 2.67263 | Correct: 047/128\n",
      "Epoch: 03 | Batch: 100 | Loss: 2.76979 | Correct: 045/128\n",
      "Epoch: 03 | Batch: 200 | Loss: 3.00376 | Correct: 038/128\n",
      "Epoch: 03 | Batch: 300 | Loss: 2.77433 | Correct: 037/128\n",
      "Epoch: 03 | Batch: 400 | Loss: 3.00723 | Correct: 038/128\n",
      "Epoch: 03 | Batch: 500 | Loss: 3.12513 | Correct: 038/128\n",
      "Epoch: 03 | Batch: 600 | Loss: 2.82879 | Correct: 038/128\n",
      "Epoch: 03 | Batch: 700 | Loss: 2.82960 | Correct: 039/128\n",
      "Epoch: 03 | Batch: 800 | Loss: 2.92637 | Correct: 033/128\n",
      "Epoch: 03 | Batch: 900 | Loss: 2.93894 | Correct: 038/128\n",
      "Epoch: 03 | Batch: 1000 | Loss: 2.97269 | Correct: 043/128\n",
      "Epoch: 03 | Batch: 1100 | Loss: 2.70068 | Correct: 049/128\n",
      "Epoch: 03 | Batch: 1200 | Loss: 3.05066 | Correct: 035/128\n",
      "Epoch: 03 | Batch: 1300 | Loss: 2.66799 | Correct: 039/128\n",
      "Epoch: 03 | Batch: 1400 | Loss: 3.08040 | Correct: 033/128\n",
      "Epoch: 03 | Batch: 1500 | Loss: 2.78049 | Correct: 037/128\n",
      "Epoch: 03 | Batch: 1600 | Loss: 2.66530 | Correct: 052/128\n",
      "Epoch: 03 | Batch: 1700 | Loss: 2.64628 | Correct: 046/128\n",
      "Epoch: 03 | Batch: 1800 | Loss: 2.90344 | Correct: 037/128\n",
      "Epoch: 03 | Batch: 1900 | Loss: 2.86522 | Correct: 036/128\n",
      "Epoch: 03 | Batch: 2000 | Loss: 3.06560 | Correct: 033/128\n",
      "Epoch: 03 | Batch: 2100 | Loss: 2.93559 | Correct: 036/128\n",
      "Epoch: 03 | Batch: 2200 | Loss: 2.94467 | Correct: 039/128\n",
      "Epoch: 03 | Batch: 2300 | Loss: 2.78163 | Correct: 042/128\n",
      "Epoch: 03 | Batch: 2400 | Loss: 3.01848 | Correct: 034/128\n",
      "Epoch: 03 | Batch: 2500 | Loss: 2.84336 | Correct: 036/128\n",
      "Epoch: 03 | Batch: 2600 | Loss: 2.71745 | Correct: 042/128\n",
      "Epoch: 03 | Batch: 2700 | Loss: 2.66139 | Correct: 043/128\n",
      "Epoch: 03 | Batch: 2800 | Loss: 2.81408 | Correct: 042/128\n",
      "Epoch: 03 | Batch: 2900 | Loss: 2.67543 | Correct: 045/128\n",
      "Epoch: 03 | Batch: 3000 | Loss: 2.89634 | Correct: 041/128\n",
      "Epoch: 03 | Batch: 3100 | Loss: 2.84997 | Correct: 040/128\n",
      "Epoch: 03 | Batch: 3200 | Loss: 2.73259 | Correct: 043/128\n",
      "Epoch: 03 | Batch: 3300 | Loss: 2.78728 | Correct: 044/128\n",
      "Epoch: 03 | Batch: 3400 | Loss: 2.76041 | Correct: 047/128\n",
      "Epoch: 03 | Batch: 3500 | Loss: 2.70419 | Correct: 047/128\n",
      "Epoch: 03 | Batch: 3600 | Loss: 2.86507 | Correct: 038/128\n",
      "Epoch: 03 | Batch: 3700 | Loss: 3.16922 | Correct: 029/128\n",
      "Epoch: 03 | Batch: 3800 | Loss: 2.71006 | Correct: 041/128\n",
      "Epoch: 03 | Batch: 3900 | Loss: 2.86766 | Correct: 039/128\n",
      "Epoch: 03 | Batch: 4000 | Loss: 3.09504 | Correct: 038/128\n",
      "Epoch: 03 | Batch: 4100 | Loss: 2.82444 | Correct: 042/128\n",
      "Epoch: 03 | Batch: 4200 | Loss: 2.64387 | Correct: 045/128\n",
      "Epoch: 03 | Batch: 4300 | Loss: 2.84890 | Correct: 043/128\n",
      "Epoch: 03 | Batch: 4400 | Loss: 2.60248 | Correct: 050/128\n",
      "Epoch: 03 | Batch: 4500 | Loss: 2.74263 | Correct: 042/128\n",
      "Epoch: 03 | Batch: 4600 | Loss: 2.90274 | Correct: 036/128\n",
      "Epoch: 03 | Batch: 4700 | Loss: 3.15796 | Correct: 029/128\n",
      "Epoch: 03 | Batch: 4800 | Loss: 2.52233 | Correct: 047/128\n",
      "Epoch: 03 | Batch: 4900 | Loss: 2.99724 | Correct: 034/128\n",
      "Epoch: 03 | Batch: 5000 | Loss: 3.07358 | Correct: 038/128\n",
      "Epoch: 03 | Batch: 5100 | Loss: 3.19869 | Correct: 027/128\n",
      "Epoch: 03 | Batch: 5200 | Loss: 2.85795 | Correct: 034/128\n",
      "Epoch: 03 | Batch: 5300 | Loss: 2.53573 | Correct: 048/128\n",
      "Epoch: 03 | Batch: 5400 | Loss: 2.69557 | Correct: 041/128\n",
      "Epoch: 03 | Batch: 5500 | Loss: 2.77357 | Correct: 037/128\n",
      "\n",
      "Epoch: 03 | Testing Accuracy: 53337.0/178167 (29.937%) |  Historical Best: 29.937%\n",
      "\n",
      "Epoch: 04 | Batch: 000 | Loss: 2.80214 | Correct: 044/128\n",
      "Epoch: 04 | Batch: 100 | Loss: 2.85513 | Correct: 043/128\n",
      "Epoch: 04 | Batch: 200 | Loss: 2.89778 | Correct: 031/128\n",
      "Epoch: 04 | Batch: 300 | Loss: 2.55788 | Correct: 054/128\n",
      "Epoch: 04 | Batch: 400 | Loss: 2.79064 | Correct: 033/128\n",
      "Epoch: 04 | Batch: 500 | Loss: 2.83451 | Correct: 046/128\n",
      "Epoch: 04 | Batch: 600 | Loss: 2.99705 | Correct: 046/128\n",
      "Epoch: 04 | Batch: 700 | Loss: 2.68463 | Correct: 039/128\n",
      "Epoch: 04 | Batch: 800 | Loss: 3.03807 | Correct: 038/128\n",
      "Epoch: 04 | Batch: 900 | Loss: 2.67577 | Correct: 044/128\n",
      "Epoch: 04 | Batch: 1000 | Loss: 2.62098 | Correct: 048/128\n",
      "Epoch: 04 | Batch: 1100 | Loss: 2.90431 | Correct: 037/128\n",
      "Epoch: 04 | Batch: 1200 | Loss: 2.96302 | Correct: 035/128\n",
      "Epoch: 04 | Batch: 1300 | Loss: 2.74202 | Correct: 045/128\n",
      "Epoch: 04 | Batch: 1400 | Loss: 2.72958 | Correct: 046/128\n",
      "Epoch: 04 | Batch: 1500 | Loss: 3.00545 | Correct: 031/128\n",
      "Epoch: 04 | Batch: 1600 | Loss: 2.62400 | Correct: 039/128\n",
      "Epoch: 04 | Batch: 1700 | Loss: 2.74807 | Correct: 049/128\n",
      "Epoch: 04 | Batch: 1800 | Loss: 3.38637 | Correct: 030/128\n",
      "Epoch: 04 | Batch: 1900 | Loss: 2.70768 | Correct: 052/128\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 11\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# train_loader, test_loader = load_two_dataset(args, 'data/fft_result_wo_simplify/trnX_norm.pkl', 'data/fft_result_wo_simplify/trnY_specialty.pkl')\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# train_two_specialty(net, args, train_loader, test_loader)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m train_loader, test_loader \u001b[38;5;241m=\u001b[39m load_my_dataset(args, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/fft_result_wo_simplify/trnX.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/fft_result_wo_simplify/trnY.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain_single_col\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn [2], line 20\u001b[0m, in \u001b[0;36mtrain_single_col\u001b[1;34m(model, args, train_loader, test_loader)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (data, label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m     22\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     23\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mreshape(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=624'>625</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=625'>626</a>\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=626'>627</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=627'>628</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=628'>629</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=629'>630</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=630'>631</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=631'>632</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=668'>669</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=669'>670</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=670'>671</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=671'>672</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataloader.py?line=672'>673</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=55'>56</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=56'>57</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=57'>58</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=58'>59</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=59'>60</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=55'>56</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=56'>57</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=57'>58</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=58'>59</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/_utils/fetch.py?line=59'>60</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataset.py?line=292'>293</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataset.py?line=293'>294</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/utils/data/dataset.py?line=294'>295</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "File \u001b[1;32mc:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py:15\u001b[0m, in \u001b[0;36mMyDataSet.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/tld92/OneDrive/Code/AAI_Proj/script/DataSet.py?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m     <a href='file:///c%3A/Users/tld92/OneDrive/Code/AAI_Proj/script/DataSet.py?line=13'>14</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrnX[index]\n\u001b[1;32m---> <a href='file:///c%3A/Users/tld92/OneDrive/Code/AAI_Proj/script/DataSet.py?line=14'>15</a>\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mTensor(data)\n\u001b[0;32m     <a href='file:///c%3A/Users/tld92/OneDrive/Code/AAI_Proj/script/DataSet.py?line=15'>16</a>\u001b[0m     data \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     <a href='file:///c%3A/Users/tld92/OneDrive/Code/AAI_Proj/script/DataSet.py?line=16'>17</a>\u001b[0m     \u001b[39m# data = normalize(data, p=1, dim=1)\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from script.NeuralNetwork import NeuralNetwork, Args\n",
    "from torch import nn\n",
    "from script.DataSet import load_my_dataset, load_two_dataset\n",
    "\n",
    "args = Args()\n",
    "net = NeuralNetwork(250)\n",
    "# train_loader, test_loader = load_two_dataset(args, 'data/fft_result_wo_simplify/trnX_norm.pkl', 'data/fft_result_wo_simplify/trnY_specialty.pkl')\n",
    "# train_two_backbone(net, args, train_loader, test_loader)\n",
    "\n",
    "train_loader, test_loader = load_my_dataset(args, 'data/fft_result_wo_simplify/trnX.pkl', 'data/fft_result_wo_simplify/trnY.pkl')\n",
    "train_single_col(net, args, train_loader, test_loader)\n",
    "\n",
    "args = Args()\n",
    "for i in range(1,16):\n",
    "    args.name = f'specialist{i}'\n",
    "    net = NeuralNetwork(250)\n",
    "    train_loader, test_loader = load_two_dataset(args, f'data/fft_result_wo_simplify/trnX_sp{i}.pkl', f'data/fft_result_wo_simplify/trnY_sp{i}.pkl')\n",
    "    train_two_specialty(net, args, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trnX = pd.read_pickle('data/fft_result_wo_simplify/trnX.pkl')\n",
    "trnY = pd.read_pickle('data/fft_result_wo_simplify/trnY_specialty.pkl')\n",
    "for i in range(0,1):\n",
    "    temp_y = trnY[trnY[:,1] == i]\n",
    "    temp_x = trnX[trnY[:,1] == i]\n",
    "    pd.to_pickle(temp_x, f'data/fft_result_wo_simplify/trnX_sp{i}.pkl')\n",
    "    pd.to_pickle(temp_y, f'data/fft_result_wo_simplify/trnY_sp{i}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0,1,3,9,12,14,15\n",
    "net = NeuralNetwork(250)\n",
    "torch.save(net, f'model/specialist{2}.pt')\n",
    "torch.save(net, f'model/specialist{4}.pt')\n",
    "torch.save(net, f'model/specialist{5}.pt')\n",
    "torch.save(net, f'model/specialist{6}.pt')\n",
    "torch.save(net, f'model/specialist{7}.pt')\n",
    "torch.save(net, f'model/specialist{8}.pt')\n",
    "torch.save(net, f'model/specialist{10}.pt')\n",
    "torch.save(net, f'model/specialist{11}.pt')\n",
    "torch.save(net, f'model/specialist{13}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "backbone = torch.load(f'model/backbone.pt', map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 127] 找不到指定的程序。\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from script.NeuralNetwork import NeuralNetwork, Args\n",
    "from torch import nn\n",
    "from script.DataSet import load_spec_dataset, load_two_dataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "backbone = torch.load(f'model/backbone.pt', map_location=torch.device('cpu'))\n",
    "args = Args()\n",
    "train_loader, test_loader = load_two_dataset(args, 'data/fft_result_wo_simplify/trnX.pkl', 'data/fft_result_wo_simplify/trnY_specialty.pkl')\n",
    "total_data = []\n",
    "total_label = []\n",
    "total_specialty = []\n",
    "pred_specialty = []\n",
    "del train_loader\n",
    "for batch_idx, (data, label, specialty) in enumerate(test_loader):\n",
    "\n",
    "        batch_size = data.size()[0]\n",
    "        data = data.reshape(batch_size, -1)\n",
    "        \n",
    "        output = backbone.forward(data)\n",
    "        output = output.reshape(batch_size, -1)\n",
    "        output = F.softmax(output, dim=1)\n",
    "\n",
    "        pred_spec = output.data.max(1)[1]\n",
    "        \n",
    "        total_data = total_data + data.tolist()\n",
    "        total_label = total_label + label.tolist()\n",
    "        total_specialty = total_specialty + specialty.tolist()\n",
    "        pred_specialty = pred_specialty + pred_spec.tolist()\n",
    "\n",
    "df = pd.DataFrame({'data': total_data,\n",
    "                    'label': total_label, \n",
    "                    'specialty': total_specialty,\n",
    "                    'pred_spec': pred_specialty})\n",
    "torch.save(df, 'data/predict_temp.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 127] 找不到指定的程序。\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\tld92\\AppData\\Local\\Temp\\ipykernel_3828\\2201740341.py\", line 21, in <module>\n",
      "    train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
      "  File \"c:\\Users\\tld92\\OneDrive\\Code\\AAI_Proj\\script\\DataSet.py\", line 123, in load_spec_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 344, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 107, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from script.NeuralNetwork import NeuralNetwork, Args\n",
    "from torch import nn\n",
    "from script.DataSet import load_spec_dataset, load_two_dataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "df = torch.load('data/predict_temp.pt')\n",
    "total_data = df['data'].values\n",
    "total_label = df['label'].values\n",
    "total_specialty = df['specialty'].values\n",
    "pred_specialty = df['pred_spec'].values\n",
    "del df\n",
    "args = Args()\n",
    "result = pd.DataFrame(columns=[\"data\", \"label\", \"specialty\", \"pred_label\", \"pred_spec\"])\n",
    "for i in range(0,16):\n",
    "    specialist = torch.load(f'model/specialist{i}.pt', map_location=torch.device('cpu'))\n",
    "    select = pred_specialty == i\n",
    "    try:\n",
    "        train_loader, test_loader = load_spec_dataset(args, total_data[select], total_label[select], total_specialty[select], pred_specialty[select])\n",
    "        for data_loader in [train_loader, test_loader]:\n",
    "            for batch_idx, (data, label, specialty, pred_spec) in enumerate(data_loader):\n",
    "\n",
    "                batch_size = data.size()[0]\n",
    "                data = data.reshape(batch_size, -1)\n",
    "                \n",
    "                output = specialist.forward(data)\n",
    "                output = output.reshape(batch_size, -1)\n",
    "                output = F.softmax(output, dim=1)\n",
    "\n",
    "                pred_label = output.data.max(1)[1]\n",
    "\n",
    "                rows = {'data': data.tolist(),\n",
    "                        'label': label.reshape(-1).tolist(),\n",
    "                        'specialty': specialty.reshape(-1).tolist(),\n",
    "                        'pred_label': pred_label.reshape(-1).tolist(),\n",
    "                        'pred_spec': pred_spec.reshape(-1).tolist()}\n",
    "                result = pd.concat([result, pd.DataFrame(rows)], ignore_index=True)\n",
    "    except Exception:\n",
    "        print(traceback.print_exc())\n",
    "\n",
    "torch.save(result, 'data/prediction.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'data\\x0cft_result_wo_simplify\\trnY.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;130;43;01m\\f\u001b[39;49;00m\u001b[38;5;124;43mft_result_wo_simplify\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[38;5;124;43mrnY.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=767'>768</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m pickle_load_args\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=768'>769</a>\u001b[0m     pickle_load_args[\u001b[39m'\u001b[39m\u001b[39mencoding\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=770'>771</a>\u001b[0m \u001b[39mwith\u001b[39;00m _open_file_like(f, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m opened_file:\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=771'>772</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=772'>773</a>\u001b[0m         \u001b[39m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=773'>774</a>\u001b[0m         \u001b[39m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=774'>775</a>\u001b[0m         \u001b[39m# reset back to the original position.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=775'>776</a>\u001b[0m         orig_position \u001b[39m=\u001b[39m opened_file\u001b[39m.\u001b[39mtell()\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=267'>268</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=268'>269</a>\u001b[0m     \u001b[39mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=269'>270</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=270'>271</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=271'>272</a>\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m mode:\n",
      "File \u001b[1;32mD:\\Anaconda3\\envs\\python38\\lib\\site-packages\\torch\\serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=249'>250</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, name, mode):\n\u001b[1;32m--> <a href='file:///d%3A/Anaconda3/envs/python38/lib/site-packages/torch/serialization.py?line=250'>251</a>\u001b[0m     \u001b[39msuper\u001b[39m(_open_file, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mopen\u001b[39;49m(name, mode))\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'data\\x0cft_result_wo_simplify\\trnY.pkl'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "prediction = torch.load('data/prediction.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6057687450537979"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[prediction['label'] == prediction['pred_label']].shape[0] / prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from script.NeuralNetwork import NeuralNetwork, Args\n",
    "from torch import nn\n",
    "from script.DataSet import load_my_dataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "backbone = torch.load(f'model/backbone.pt', map_location=torch.device('cpu'))\n",
    "args = Args()\n",
    "train_loader, test_loader = load_my_dataset(args, 'data/val/trnX.pkl', 'data/val/trnY.pkl')\n",
    "total_data = []\n",
    "total_label = []\n",
    "pred_specialty = []\n",
    "for data_loader in [train_loader, test_loader]:\n",
    "    for batch_idx, (data, label) in enumerate(data_loader):\n",
    "\n",
    "        batch_size = data.size()[0]\n",
    "        data = data.reshape(batch_size, -1)\n",
    "        \n",
    "        output = backbone.forward(data)\n",
    "        output = output.reshape(batch_size, -1)\n",
    "        output = F.softmax(output, dim=1)\n",
    "\n",
    "        pred_spec = output.data.max(1)[1]\n",
    "        \n",
    "        total_data = total_data + data.tolist()\n",
    "        total_label = total_label + list(label)\n",
    "        pred_specialty = pred_specialty + pred_spec.tolist()\n",
    "\n",
    "df = pd.DataFrame({'data': total_data,\n",
    "                    'label': total_label, \n",
    "                    'pred_spec': pred_specialty})\n",
    "torch.save(df, 'data/predict_temp_val_noise.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Lidan\\AppData\\Local\\Temp\\ipykernel_38852\\71656649.py\", line 20, in <cell line: 16>\n",
      "    train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
      "  File \"d:\\研究生\\高级人工智能\\proj\\script\\DataSet.py\", line 164, in load_three_dataset\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 277, in __init__\n",
      "    sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n",
      "  File \"D:\\ProgramData\\Anaconda3\\envs\\python3812\\lib\\site-packages\\torch\\utils\\data\\sampler.py\", line 97, in __init__\n",
      "    raise ValueError(\"num_samples should be a positive integer \"\n",
      "ValueError: num_samples should be a positive integer value, but got num_samples=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "from script.NeuralNetwork import NeuralNetwork, Args\n",
    "from torch import nn\n",
    "from script.DataSet import load_three_dataset\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "df = torch.load('data/predict_temp_val_noise.pt')\n",
    "total_data = df['data'].values\n",
    "total_label = df['label'].values\n",
    "pred_specialty = df['pred_spec'].values\n",
    "del df\n",
    "args = Args()\n",
    "result = pd.DataFrame(columns=[\"data\", \"label\", \"pred_label\", \"pred_spec\"])\n",
    "for i in range(0,16):\n",
    "    specialist = torch.load(f'model/specialist{i}.pt', map_location=torch.device('cpu'))\n",
    "    select = pred_specialty == i\n",
    "    try:\n",
    "        train_loader, test_loader = load_three_dataset(args, total_data[select], total_label[select], pred_specialty[select])\n",
    "        for data_loader in [train_loader, test_loader]:\n",
    "            for batch_idx, (data, label, pred_spec) in enumerate(data_loader):\n",
    "\n",
    "                batch_size = data.size()[0]\n",
    "                data = data.reshape(batch_size, -1)\n",
    "                \n",
    "                output = specialist.forward(data)\n",
    "                output = output.reshape(batch_size, -1)\n",
    "                output = F.softmax(output, dim=1)\n",
    "\n",
    "                pred_label = output.data.max(1)[1]\n",
    "\n",
    "                rows = {'data': data.tolist(),\n",
    "                        'label': list(label),\n",
    "                        'pred_label': pred_label.reshape(-1).tolist(),\n",
    "                        'pred_spec': list(pred_spec)}\n",
    "                result = pd.concat([result, pd.DataFrame(rows)], ignore_index=True)\n",
    "    except Exception:\n",
    "        print(traceback.print_exc())\n",
    "\n",
    "torch.save(result, 'data/predict_val_noise.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "prediction = torch.load('data/predict_test.pt')\n",
    "prediction_noise = torch.load('data/predict_test_noise.pt')\n",
    "\n",
    "result = []\n",
    "for sample_name in sorted(prediction['label'].unique()):\n",
    "    temp_df = prediction[prediction['label'] == sample_name]\n",
    "    id = temp_df['pred_label'].mode()[0]\n",
    "    result.append([f\"{sample_name}.flac\", f\"spk{id+1:03n}\"])\n",
    "\n",
    "for sample_name in sorted(prediction_noise['label'].unique()):\n",
    "    temp_df = prediction_noise[prediction_noise['label'] == sample_name]\n",
    "    id = temp_df['pred_label'].mode()[0]\n",
    "    result.append([f\"{sample_name}.flac\", f\"spk{id+1:03n}\"])\n",
    "\n",
    "with open('data/result.txt', 'w') as f:\n",
    "    for item in result:\n",
    "        f.write(f\"{item[0]} {item[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "prediction = torch.load('data/predict_val_noise.pt')\n",
    "prediction['pred_label'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>label</th>\n",
       "      <th>pred_label</th>\n",
       "      <th>pred_spec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108855</th>\n",
       "      <td>[1.3763682842254639, 0.623872697353363, 1.4151...</td>\n",
       "      <td>spk045</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor(14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213001</th>\n",
       "      <td>[1.2607989311218262, 0.5374980568885803, 1.009...</td>\n",
       "      <td>spk001</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor(14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95687</th>\n",
       "      <td>[0.932559609413147, 0.8476085066795349, 0.8821...</td>\n",
       "      <td>spk249</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor(14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110686</th>\n",
       "      <td>[0.16487714648246765, 0.19675776362419128, 0.2...</td>\n",
       "      <td>spk198</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor(14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206283</th>\n",
       "      <td>[0.296858549118042, 0.6865970492362976, 0.9847...</td>\n",
       "      <td>spk090</td>\n",
       "      <td>1</td>\n",
       "      <td>tensor(14)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8746</th>\n",
       "      <td>[10.543007850646973, 6.4660258293151855, 10.75...</td>\n",
       "      <td>spk250</td>\n",
       "      <td>250</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5480</th>\n",
       "      <td>[4.188472270965576, 4.261137962341309, 6.09306...</td>\n",
       "      <td>spk200</td>\n",
       "      <td>250</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8758</th>\n",
       "      <td>[7.1447834968566895, 6.119589805603027, 6.3125...</td>\n",
       "      <td>spk232</td>\n",
       "      <td>250</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8839</th>\n",
       "      <td>[3.150238275527954, 5.254030227661133, 8.06184...</td>\n",
       "      <td>spk250</td>\n",
       "      <td>250</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13404</th>\n",
       "      <td>[5.0530619621276855, 5.880580425262451, 0.5718...</td>\n",
       "      <td>spk250</td>\n",
       "      <td>250</td>\n",
       "      <td>tensor(0)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>217712 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     data   label pred_label  \\\n",
       "108855  [1.3763682842254639, 0.623872697353363, 1.4151...  spk045          1   \n",
       "213001  [1.2607989311218262, 0.5374980568885803, 1.009...  spk001          1   \n",
       "95687   [0.932559609413147, 0.8476085066795349, 0.8821...  spk249          1   \n",
       "110686  [0.16487714648246765, 0.19675776362419128, 0.2...  spk198          1   \n",
       "206283  [0.296858549118042, 0.6865970492362976, 0.9847...  spk090          1   \n",
       "...                                                   ...     ...        ...   \n",
       "8746    [10.543007850646973, 6.4660258293151855, 10.75...  spk250        250   \n",
       "5480    [4.188472270965576, 4.261137962341309, 6.09306...  spk200        250   \n",
       "8758    [7.1447834968566895, 6.119589805603027, 6.3125...  spk232        250   \n",
       "8839    [3.150238275527954, 5.254030227661133, 8.06184...  spk250        250   \n",
       "13404   [5.0530619621276855, 5.880580425262451, 0.5718...  spk250        250   \n",
       "\n",
       "         pred_spec  \n",
       "108855  tensor(14)  \n",
       "213001  tensor(14)  \n",
       "95687   tensor(14)  \n",
       "110686  tensor(14)  \n",
       "206283  tensor(14)  \n",
       "...            ...  \n",
       "8746     tensor(0)  \n",
       "5480     tensor(0)  \n",
       "8758     tensor(0)  \n",
       "8839     tensor(0)  \n",
       "13404    tensor(0)  \n",
       "\n",
       "[217712 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.sort_values(by='pred_label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217712"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for index, row in prediction.iterrows():\n",
    "  label = int(row['label'][3:])\n",
    "  pred = row['pred_label']\n",
    "  if label == pred:\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47570184463878884"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt / prediction.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_pickle('data/trnY_sp15.pkl')\n",
    "len(np.unique(data[:,0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5251d2f2824344d0e880f106347f3c7075b119d5c137feaa5c66814814206cd8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python3812')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
